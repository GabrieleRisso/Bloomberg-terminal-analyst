#! /bin/bash

app="/GamestonkTerminal"
rootpath="$PWD"

#variables
export today=$(date -d "today" +"%Y/%m/%d")
export 1month=$(date -d "30 days ago" +"%Y/%m/%d")
export 3montago=$(date -d "90 days ago" +"%Y/%m/%d")

#move this script in game stonkterminal folder. make it executbale with "chmod +x stocksh"

#limitgain=5 #limit the number of gainers stocks displayed

#Active
#Arkord
#Asc
#Cnews
#Divcal
#Fds
#Fipo
#Ford
#OK - Gainers
#OK - Gtech
#Hotpenny
#Losers
#Lowfloat
#Pipo
#Rtat
#Trending
#Ugs
#Ulc
#Upcoming


lines=$(python ~/GamestonkTerminal/terminal.py <<END
/stocks/disc/gainers -l 5 --export csv/../quit/exit
END
)

mkdir ${HOME}/dailyreport

dir="{HOME}/dailyreport/$i"




path=$(echo "$lines" | grep -Eo  "${HOME}/GamestonkTerminal/exports/stocks/discovery/gainers_[0-9]*_[0-9]*.csv" | head -n 1)

echo "$path"

#nasdaq	10	2	10	810	75.21%
#nasdaq	10	2	5	618	57.38%
#nasdaq	10	3	10	358	33.24%
#nasdaq	10	3	5	262	24.33%
#amex	10	2	10	70	49.30%
#amex	10	2	5	62	43.66%
#nasdaq	100	2	10	271	25.16%
#nasdaq	100	2	5	196	18.20%
#nasdaq	100	3	10	97	9.01%
#nasdaq	10	2	10	810	75.21%
#nasdaq	10	2	5	618	57.38%
#nasdaq	10	3	10	358	33.24%
#nasdaq	10	3	5	262	24.33%





#behiviural analisis 
#Bypass any paywall,

#https://12ft.io/<URL>


#https://github.com/CardosoJr/investing/tree/9cc20872c83037c2f7a17c62b47d3b4798f3a46f/Extractors/news
#gnews
#twitter
#reddit
#finhub (finnhub.io)
#financialmodelingprep.com
#newsapi.org
#stocktwits.com
#finbrain.tech

#Bullbear -> done
#Getdd
#Headlines
#Hist
#Infer
#Mentions -> to be performed on a restricted stock number since ythere is no export possibility
#Messages
#Popular
#Ã Queries
#Regions
#Rise
#entiment
#Spac
#Spac C
#Stalker
#Stats
#Trend
#Trending
#Watchlist
#Wsb #not really intresting for now-

#the ba that require load of stock:

#  bullbear      estimate quick sentiment from last 30 messages on board                    ?
#?     messages      output up to the 30 last messages on the board                             ?
#? [Twitter]                                                                                    ?
#?              infer about stock's sentiment from latest tweets                           ?
#?     sentiment     in-depth sentiment prediction from tweets over time                        ?
#? [Google]                                                                                     ?
#?     mentions      interest over time based on stock's mentions                               ?
#?     regions       regions that show highest interest in stock                                ?
#?     queries       top related queries with this stock                                        ?
#?     rise          top rising related queries with stock        
#hist

function mentions13_14 {
#usage: mentions [-s START] [-h]
#Uses Google metrics to plot ticker mentions over time.

#optional arguments:
#  -s START, --start START
#                        starting date (format YYYY-MM-DD) from when we are interested in stock's mentions. (default: 2020-09-15 00:00:00)
mention_start="2018-01-01"
output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/mention -s ${mention_start} --export cvs/exit`
outputpng=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/mention -s ${mention_start} --export png/exit`

mention_path=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/mention_[0-9]*_[0-9]*.csv" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat
mention_pngpath=$(echo "$outputpng" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/mention_[0-9]*_[0-9]*.png" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat

mv $mention_pngpath "${dir}"/mentions.png

mention_value=($(awk -F "\"*,\"*" '{print $2}' ${mention_path} ))
mention_date=($(awk -F "\"*,\"*" '{print $1}' ${mention_path} ))

#neede function to produce an index that express how hot is the query in the last week.
#Weighted Mean = ·ni=1 (xi*wi)/·ni=1wi

awk -v d="${mention_value[-1]}" -F"," 'BEGIN { OFS = "," } {$16=d; print}' ba13.csv > ba14.csv #add present day value of mentions with no respect to the past

}

function rise10_13 {
#usage: rise [-l LIMIT] [-h] [--export {csv,json,xlsx,png,jpg,pdf,svg}]
#Print top rising related queries with this stockÕs query. [Source: Google]

#optional arguments:
#  -l LIMIT, --limit LIMIT
#                        limit of top rising related queries to print. (default: 10)
#  -h, --help            show this help message (default: False)
#  --export {csv,json,xlsx,png,jpg,pdf,svg}
                      #  Export raw data into csv, json, xlsx and figure into png, jpg, pdf, svg (default: )
#Example:

#2022 Feb 16, 10:40 (?) /stocks/ba/ $ rise
#Top rising AAPL's related queries
#????????????????????????????
#? query           ? value  ?
#????????????????????????????
#? nio stock       ? 227850 ?
#????????????????????????????
#? nio             ? 183950 ?
#????????????????????????????
#? pltr            ? 103100 ?


#,query,value
#0,nio stock,485050
#1,nio,358800
#2,pltr,166750
#3,pltr stock,130000
#4,zm,104250

rise_limit=6
output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/rise -l ${rise_limit} --export cvs`
outputpng=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/rise -l ${rise_limit} --export png`

rise_path=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/rise_[0-9]*_[0-9]*.csv" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat
rise_pngpath=$(echo "$outputpng" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/rise_[0-9]*_[0-9]*.png" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat

mv $rise_pngpath "${dir}"/rise.png  

rise_queri=($(awk -F "\"*,\"*" '{print $1}' ${rise_path} ))  #to check clumn number 0 or 1 
#rise_value=($(awk -F "\"*,\"*" '{print $2}' ${rise_path} )) 

#now i removee word like stock or price or duplicate to the sting in order to extract new stocks
#skip title
for j in {1 .. $rise_limit}
	do
	rise_queri="${ries_queri#stock}"; rise_queri="${rise_queri#price}" ; rise_queri=("${rise_queri/$i}");
#if two follwing quari are the same after the revoval of price and stock do -> move array back 1
	done
rise_queri_noduplicate=($(echo "${rise_queri[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' '))

awk -v d="${rise_queri_noduplicate[1]}" -F"," 'BEGIN { OFS = "," } {$13=d; print}' ba10.csv > ba11.csv
awk -v d="${rise_queri_noduplicate[2]}" -F"," 'BEGIN { OFS = "," } {$14=d; print}' ba11.csv > ba12.csv
awk -v d="${rise_queri_noduplicate[3]}" -F"," 'BEGIN { OFS = "," } {$15=d; print}' ba12.csv > ba13.csv

}


function queries13_16 {
#usage: queries [-l LIMIT] [-h] [--export {csv,json,xlsx,png,jpg,pdf,svg}]
#Print top related queries with this stockÕs query. [Source: Google]

#optional arguments:
#  -l LIMIT, --limit LIMIT
                        #limit of top related queries to print. (default: 10)
#  -h, --help            show this help message (default: False)
#  --export {csv,json,xlsx,png,jpg,pdf,svg}
#                        Export raw data into csv, json, xlsx and figure into png, jpg, pdf, svg (default: )

# Top AMZN's related queries
#????????????????????????????
#? query            ? value ?
#????????????????????????????
#? amzn stock       ? 100%  ?
#????????????????????????????
#? amzn price       ? 31%   ?
queri_limit=5 #num of output related query
output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/queries -l ${queri_limit} --export cvs`
#outputpng=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/rise -l ${rise_limit} --export png`
#rise_path=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/query_[0-9]*_[0-9]*.csv" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat
#rise_pngpath=$(echo "$outputpng" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/query_[0-9]*_[0-9]*.png" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat
#mv $rise_pngpath "${dir}"/rise.png  

#extract the terminal output. extract top value query names, remove excess word and genrate a new list of stock to be checked
echo ${output} > "${dir}"/queri.txt

queri_queri=($(awk -F "\"*|\"*" '{print $1}' "${dir}"/queri.txt ))  #to check clumn number 0 or 1 
queri_value=($(awk -F "\"*|\"*" '{print $2}' "${dir}"/queri.txt )) 

#now i removee word like stock or price or duplicate to the sting in order to extract new stocks
#skip title

#array=(pluto pippo)
#delete=pluto
#$ echo ${array[@]/$delete}
#pippo
#$ array=( "${array[@]/$delete}" ) #Quotes when working with string

for j in $queri_limit

	do
	#i remove stock form array fild and price and the name of the stock 
	queri_queri="${queri_queri#stock}"; queri_queri="${queri_queri#price}";  queri_queri=("${queri_queri/$i}");
#if two follwing quari are the same after the revoval of price and stock do -> move array back 1
	done
queri_queri_noduplicate=($(echo "${queri_queri[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' '))

#add the TOP 3 to the table and the others are int the quei_queri<-nodup to be used for further checks
awk -v d="${queri_queri_noduplicate[1]}" -F"," 'BEGIN { OFS = "," } {$13=d; print}' ba14.csv > ba15.csv
awk -v d="${queri_queri_noduplicate[2]}" -F"," 'BEGIN { OFS = "," } {$14=d; print}' ba15.csv > ba16.csv
awk -v d="${queri_queri_noduplicate[3]}" -F"," 'BEGIN { OFS = "," } {$15=d; print}' ba16.csv > ba17.csv

#queri_path=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/query_[0-9]*_[0-9]*.csv" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat
#output to csv doest work so :

}

function hist {
#no table output.

#the freeversion has 1000 per month this men that i a create 10 accont and switch the apy we have unlimited apy calls.
#Starter sentiment investor
#MOST POPULAR
#$19.99 	240 yearly
#   per month
#10,000 API Calls per month+Hourly metrics and breakdowns+Historical Data+Wesokets
#usage: hist [-s START] [-e END] [-n NUMBER] [-h] [--export {csv,json,xlsx,png,jpg,pdf,svg}] [--raw] [-l LIMIT]
#Plot historical sentiment data of RHI and AHI by hour. Source: [Sentiment Investor]
#AHI (Absolute Hype Index)
#AHI is a measure of how much people are talking about a stock on social media. It is calculated by dividing the total number of mentions for the chosen stock on a social network by the mean number of mentions any stock receives on that social medium.
#RHI (Relative Hype Index)
#RHI is a measure of whether people are talking about a stock more or less than usual, calculated by dividing the mean AHI for the past day by the mean AHI for for the past week for that stock.
number=50 #to make some calculation for the api calls.
output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/hist -s ${1month} -e ${today}  -n ${number} --export png`
hist_pngpath=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/hist_[0-9]*_[0-9]*.csv" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat
mv $hist_pngpath "${dir}"/hist.png 

#maybe we need to genarate the plot with pyhton and save it to he daylyreport. the terminal does it but csv or png.

}

function popular {
limit=15 #the bigger the better)
number=50 #post number for every subs
subreddits="pennystocks, RobinHoodPennyStocks, Daytrading, StockMarket, stocks, investing, wallstreetbets" #can be changed
#very time long

output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/popular -l ${limit} -n ${number} -s s{subreddit}`
#there is no export function so i do it:


echo ${output} > "${dir}"/popular.txt

pop_mentions=($(awk -F "\"*|\"*" '{print $1}' "${dir}"/popular.txt ))  #to check clumn number 0 or 1 
pop_ticker=($(awk -F "\"*|\"*" '{print $2}' "${dir}"/popular.txt ))

#if a thicker is the discover section i add 
#pop_ticker can be used insted of gianers or a combination

#I restrict the interest on only the ticer name since the other data will be produced in the anlyis
#????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#? Mentions ? Ticker ? Company                                                ? Sector               ? Price  ? Change  ? Perf Month ? URL                                  ?
#????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#? 57       ? GME    ? GameStop Corp.                                         ? Consumer Cyclical    ? 129.34 ? 2.52%   ? 8.15%      ? https://finviz.com/quote.ashx?t=GME  ?
#???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
#Popular
#usage: popular [-l LIMIT] [-n NUM] [-s S_SUBREDDIT] [-h]
#The current popular tickers on Reddit
4
#optional arguments:
#  -l LIMIT, --limit LIMIT
#                        limit of top tickers to retrieve (default: 10)
#  -n NUM, --num NUM     number of posts retrieved per sub reddit. (default: 50)
#  -s S_SUBREDDIT, --sub S_SUBREDDIT
#                        subreddits to look for tickers, e.g. pennystocks,stocks. Default: pennystocks, RobinHoodPennyStocks, Daytrading, StockMarket, stocks, investing, wallstreetbets (default: None)
}

function regions6-10 {
#usage: regions [-l LIMIT] [-h] [--export {csv,json,xlsx,png,jpg,pdf,svg}]
#Plot bars of regions based on stockÕs interest. [Source: Google]

#optional arguments:
#  -l LIMIT, --limit LIMIT
#                        limit of regions to plot that show highest interest. (default: 10)
#  -h, --help            show this help message (default: False)
#  --export {csv,json,xlsx,png,jpg,pdf,svg}
#                        Export raw data into csv, json, xlsx and figure into png, jpg, 
export region_limit=3
#Saved file: /home/gabriele/GamestonkTerminal/exports/common/behavioural_analysis/regions_20220305_155918.csv
output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/regions -l ${regions_limit} --export cvs`
output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/regions -l ${regions_limit} --export png`

region_path=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/regions_[0-9]*_[0-9]*.csv" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat
region_pngpath=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/regions_[0-9]*_[0-9]*.png" | head -n 1) #not sure head -1 is enouht to fin alwasy the file i wnat

mv $region_pngpath "${dir}"/region.png  

region_geoname=($(awk -F "\"*,\"*" '{print $1}' ${region_path} ))
region_interest=($(awk -F "\"*,\"*" '{print $2}' ${region_path} ))

[[ ! -z "$region_interest[-1]" ]] || $region_interest[-1]=-9999
[[ ! -z "$region_interest[-2]" ]] || $region_interest[-2]=-9999
[[ ! -z "$region_geoname[-1]" ]] || $region_geoname[-1]=-9999
[[ ! -z "$region_geoname[-2]" ]] || $region_geoname[-2]=-9999

#the last of the array is has the most interest

#add the two region with most interest in the stock
awk -v d="${region_geoname[-1]}" -F"," 'BEGIN { OFS = "," } {$9=d; print}' ba6.csv > ba7.csv
awk -v d="${region_interest[-1]}" -F"," 'BEGIN { OFS = "," } {$10=d; print}' ba7.csv > ba8.csv
awk -v d="${region_geoname[-2]}" -F"," 'BEGIN { OFS = "," } {$11=d; print}' ba8.csv > ba9.csv
awk -v d="${region_interest[-2]}" -F"," 'BEGIN { OFS = "," } {$12=d; print}' ba9.csv > ba10.csv

}

function bullbear1 { #depend on a stock i passaed
output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load "${i}"/bullbear/../quit/exit`
echo ${output} > "${rootpath}"/outfile.txt
#need few safty check on the psoition of the indexise. if it fails the psoision is shifted
index_bull=$(grep -Eo "[0-9]*.[0-9]" "${rootpath}"/outfile.txt)
index_bear=$(grep -Eo "[0-9]*.[0-9]" "${rootpath}"/outfile.txt)

if [ -z "$index_bull" ]
then
	echo "$i has no bullbear index"
	echo "$i,'-9999','-9999'" >> ba1.csv
else
	echo "${i} is bullish at ${index_bull} and bearish at ${index_bear}\n"
	echo "$i,${index_bull},${index_bear}" >> ba1.csv
fi
}

#python ~/GamestonkTerminal/terminal.py /stocks/ba/load "${i}"/bullbear/../quit/exit 2> ~/GamestonkTerminal/file.txt

#echo=$bullbear[@]

#bullbear=$(python ~/GamestonkTerminal/terminal.py <<END
#/stocks/load "${i}"/ba/bullbear/../quit/exit
#END
#)

#ba tabele csv
#1sotck i,2${index_bull},3${index_bear},4$headline_sentiment[0],5ScS,6AcS,7infer_highpositivesent,8infer_highnegativesent,9region_geoname#1,10region_interest#1,11region_geoname#2,12region_interest#2,13 rise quari1,14risequari2,15risequary3,16mention_value,17queri_queri_noduplicate[1],18queri_queri_noduplicate[2],19queri_queri_noduplicate[3],20

function infer2_6 {
###to ber run every hour to have relyable result to study

#infer [-l LIMIT] [-h] [--export {csv,json,xlsx}]

export ifner_limit=100  #limit of latest tweets to infer from. (default: 100)

#From: 2022-02-19 17:08:20
#To:   2022-02-19 18:04:18
#100 tweets were analyzed.
#Frequency of approx 1 tweet every 34 seconds.
#The summed compound sentiment of AAPL is: 13.2
#The average compound sentiment of AAPL is: 0.13
#Of the last 100 tweets, 45.00 % had a higher positive sentiment
#Of the last 100 tweets, 18.00 % had a higher negative sentiment



output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/infer -l ${infer_limit}`

infer_fromday=$(echo "$output" | grep -Eo  "[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]" | head -n 1)
[[ ! -z "$infer_fromday" ]] || $infer_fromday=-9999

infer_today=$(echo "$output" | grep -Eo  "[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]" | head -n 2)
[[ ! -z "$infer_fromday" ]] || $infer_fromday=-9999

infer_fromtime=$(echo "$output" | grep -Eo  "[0-9][0-9]:[0-9][0-9]:[0-9][0-9]" | head -n 1)
[[ ! -z "$infer_fromtime" ]] || $infer_fromtime=-9999

infer_totime=$(echo "$output" | grep -Eo  "[0-9][0-9]:[0-9][0-9]:[0-9][0-9]" | head -n 2)
[[ ! -z "$infer_totime" ]] || $infer_totime=-9999

infer_ScSAAPL=$(echo "$output" | grep -Eo  "[0-9]*.[0-9]*" | head -n 1) #sum
[[ ! -z "$infer_ScSAAPL" ]] || $infer_ScSAAPL=-9999

infer_AcSAAPL=$(echo "$output" | grep -Eo  "[0-9]*.[0-9]*" | head -n 2) #average
[[ ! -z "$infer_AcSAAPL" ]] || $infer_AcSAAPL=-9999

infer_highpostivesent=$(echo "$output" | grep -Eo  "[0-9]*.[0-9]*" | head -n 3) #percentage
[[ ! -z "$infer_highpostivesent" ]] || $infer_highpostivesent=-9999

infer_highnegativesent=$(echo "$output" | grep -Eo  "[0-9]*.[0-9]*" | head -n 4)
[[ ! -z "$infer_highnegativesent" ]] || $infer_highnegativesent=-9999


#add column to ba genral csv file 
awk -v d="$infer_ScSAAPL" -F"," 'BEGIN { OFS = "," } {$5=d; print}' ba2.csv > ba3.csv
awk -v d="$infer_AcSAAPL" -F"," 'BEGIN { OFS = "," } {$6=d; print}' ba3.csv > ba4.csv
awk -v d="$infer_highpostivesent" -F"," 'BEGIN { OFS = "," } {$7=d; print}' ba4.csv > ba5.csv
awk -v d="$infer_highnegativesent" -F"," 'BEGIN { OFS = "," } {$8=d; print}' ba5.csv > ba6.csv

echo "$output"

}

function sentiment {

export days=2  #number of days 1-6 in the past to extract tweets. (default: 6)
export limitsent=10 #limit of tweets to extract per hour. (default: 15) 10-60
#find out what is AAPL sentiment.
#usage: sentiment [-l LIMIT] [-d N_DAYS_PAST] [-h] [--export {csv,json,xlsx,png,jpg,pdf,svg}]

output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/sentiment -l ${limitsent} -dcommon/behavioural_analysis/sentiment ${days} --export csv/../quit/exit`
sentiment_path=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/sentiment_[0-9]*_[0-9]*.csv" | head -n 1)
##cp $sentiment_path ${HOME}/GamestonkTerminal/report

output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load ${i}/sentiment -l ${limitsent} -dcommon/behavioural_analysis/sentiment ${days} --export jpg/../quit/exit`
sentiment_path=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/sentiment_[0-9]*_[0-9]*.jpg" | head -n 1)
mv $sentiment_path  "{$dir}"/sentiment.jpg
#254,4,2022-03-03 12:23:07,RT @JasonMaPhD: $TSLA will be $2000 this year!,0.0,0.0,0.0,1.0,-0.4384,1,3,3,2022-03-03 12:23:07

#need to investigate vader polarity score and vader sentiment quest
}


function headlines2 {
output=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load "${i}"/headline --export csv/../quit/exit`
outputpng=`python ~/GamestonkTerminal/terminal.py /stocks/ba/load "${i}"/headline --export png/../quit/exit`

headline_path=$(echo "$output" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/headlines_[0-9]*_[0-9]*.csv" | head -n 1)
headline_pngpath=$(echo "$outputpng" | grep -Eo  "${HOME}/GamestonkTerminal/exports/common/behavioural_analysis/headlines_[0-9]*_[0-9]*.png" | head -n 1)

mv $headline_pngpath  "{$dir}"/headline.png


headline_date=($(awk -F "\"*,\"*" '{print $1}' ${headline_path} ))
headline_sentiment=($(awk -F "\"*,\"*" '{print $2}' ${headline_path} ))
#add column to ba1 with sentiment
awk -v d="$headline_sentiment[0]" -F"," 'BEGIN { OFS = "," } {$4=d; print}' ba1.csv > ba2.csv

#for now i will only take the the present sentiment of  the day an not use the other daa but a function to fina a whaited mean value is needed.

#actually thsi is not a good idea to takee only one value  but ok
#https://stackoverflow.com/questions/9506810/add-column-to-end-of-csv-file-using-awk-in-bash-script
#need a foor loop to do the formual
}





export i=0
gainers_record2=($(awk -F "\"*,\"*" '{print $2}' ${path} ))
for i in ${gainers_record2[@]:1} #im using gaines simbol as the best stock but we will wuse a more refined list
do

mkdir ${HOME}/dailyreport/"$i"

#ba
bullbear1
#sentiment
headlines2
infer2_6
regions6_10
rise10_13
mention13_14
queri14_17
#output=$((your command here) 2> &1)

#python yourscript 2> return_file








#mention part




done



 
 
 
 